{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from gpt import GPTLanguageModel, get_batch, optimizer, estimate_loss\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from torch.quantization import quantize_dynamic\n",
    "from icecream import ic\n",
    "# torch.set_printoptions(precision=40, sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model for generation...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"fully_train.pth\"\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Model loading for generation\n",
    "print(\"Loading the model for generation...\")\n",
    "checkpoint = torch.load(model_save_path, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "token_embedding_table.weight: torch.Size([65, 384])\n",
      "token_embedding_table.weight: torch.float32\n",
      "position_embedding_table.weight: torch.Size([256, 384])\n",
      "position_embedding_table.weight: torch.float32\n",
      "blocks.0.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.0.key.weight: torch.float32\n",
      "blocks.0.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.0.query.weight: torch.float32\n",
      "blocks.0.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.0.value.weight: torch.float32\n",
      "blocks.0.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.1.key.weight: torch.float32\n",
      "blocks.0.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.1.query.weight: torch.float32\n",
      "blocks.0.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.1.value.weight: torch.float32\n",
      "blocks.0.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.2.key.weight: torch.float32\n",
      "blocks.0.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.2.query.weight: torch.float32\n",
      "blocks.0.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.2.value.weight: torch.float32\n",
      "blocks.0.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.3.key.weight: torch.float32\n",
      "blocks.0.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.3.query.weight: torch.float32\n",
      "blocks.0.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.3.value.weight: torch.float32\n",
      "blocks.0.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.4.key.weight: torch.float32\n",
      "blocks.0.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.4.query.weight: torch.float32\n",
      "blocks.0.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.4.value.weight: torch.float32\n",
      "blocks.0.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.5.key.weight: torch.float32\n",
      "blocks.0.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.5.query.weight: torch.float32\n",
      "blocks.0.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.0.sa.heads.5.value.weight: torch.float32\n",
      "blocks.0.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.0.sa.proj.weight: torch.float32\n",
      "blocks.0.sa.proj.bias: torch.Size([384])\n",
      "blocks.0.sa.proj.bias: torch.float32\n",
      "blocks.0.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.0.ffwd.net.0.weight: torch.float32\n",
      "blocks.0.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.0.ffwd.net.0.bias: torch.float32\n",
      "blocks.0.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.0.ffwd.net.2.weight: torch.float32\n",
      "blocks.0.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.0.ffwd.net.2.bias: torch.float32\n",
      "blocks.0.ln1.weight: torch.Size([384])\n",
      "blocks.0.ln1.weight: torch.float32\n",
      "blocks.0.ln1.bias: torch.Size([384])\n",
      "blocks.0.ln1.bias: torch.float32\n",
      "blocks.0.ln2.weight: torch.Size([384])\n",
      "blocks.0.ln2.weight: torch.float32\n",
      "blocks.0.ln2.bias: torch.Size([384])\n",
      "blocks.0.ln2.bias: torch.float32\n",
      "blocks.1.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.0.key.weight: torch.float32\n",
      "blocks.1.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.0.query.weight: torch.float32\n",
      "blocks.1.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.0.value.weight: torch.float32\n",
      "blocks.1.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.1.key.weight: torch.float32\n",
      "blocks.1.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.1.query.weight: torch.float32\n",
      "blocks.1.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.1.value.weight: torch.float32\n",
      "blocks.1.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.2.key.weight: torch.float32\n",
      "blocks.1.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.2.query.weight: torch.float32\n",
      "blocks.1.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.2.value.weight: torch.float32\n",
      "blocks.1.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.3.key.weight: torch.float32\n",
      "blocks.1.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.3.query.weight: torch.float32\n",
      "blocks.1.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.3.value.weight: torch.float32\n",
      "blocks.1.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.4.key.weight: torch.float32\n",
      "blocks.1.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.4.query.weight: torch.float32\n",
      "blocks.1.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.4.value.weight: torch.float32\n",
      "blocks.1.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.5.key.weight: torch.float32\n",
      "blocks.1.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.5.query.weight: torch.float32\n",
      "blocks.1.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.1.sa.heads.5.value.weight: torch.float32\n",
      "blocks.1.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.1.sa.proj.weight: torch.float32\n",
      "blocks.1.sa.proj.bias: torch.Size([384])\n",
      "blocks.1.sa.proj.bias: torch.float32\n",
      "blocks.1.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.1.ffwd.net.0.weight: torch.float32\n",
      "blocks.1.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.1.ffwd.net.0.bias: torch.float32\n",
      "blocks.1.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.1.ffwd.net.2.weight: torch.float32\n",
      "blocks.1.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.1.ffwd.net.2.bias: torch.float32\n",
      "blocks.1.ln1.weight: torch.Size([384])\n",
      "blocks.1.ln1.weight: torch.float32\n",
      "blocks.1.ln1.bias: torch.Size([384])\n",
      "blocks.1.ln1.bias: torch.float32\n",
      "blocks.1.ln2.weight: torch.Size([384])\n",
      "blocks.1.ln2.weight: torch.float32\n",
      "blocks.1.ln2.bias: torch.Size([384])\n",
      "blocks.1.ln2.bias: torch.float32\n",
      "blocks.2.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.0.key.weight: torch.float32\n",
      "blocks.2.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.0.query.weight: torch.float32\n",
      "blocks.2.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.0.value.weight: torch.float32\n",
      "blocks.2.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.1.key.weight: torch.float32\n",
      "blocks.2.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.1.query.weight: torch.float32\n",
      "blocks.2.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.1.value.weight: torch.float32\n",
      "blocks.2.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.2.key.weight: torch.float32\n",
      "blocks.2.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.2.query.weight: torch.float32\n",
      "blocks.2.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.2.value.weight: torch.float32\n",
      "blocks.2.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.3.key.weight: torch.float32\n",
      "blocks.2.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.3.query.weight: torch.float32\n",
      "blocks.2.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.3.value.weight: torch.float32\n",
      "blocks.2.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.4.key.weight: torch.float32\n",
      "blocks.2.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.4.query.weight: torch.float32\n",
      "blocks.2.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.4.value.weight: torch.float32\n",
      "blocks.2.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.5.key.weight: torch.float32\n",
      "blocks.2.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.5.query.weight: torch.float32\n",
      "blocks.2.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.2.sa.heads.5.value.weight: torch.float32\n",
      "blocks.2.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.2.sa.proj.weight: torch.float32\n",
      "blocks.2.sa.proj.bias: torch.Size([384])\n",
      "blocks.2.sa.proj.bias: torch.float32\n",
      "blocks.2.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.2.ffwd.net.0.weight: torch.float32\n",
      "blocks.2.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.2.ffwd.net.0.bias: torch.float32\n",
      "blocks.2.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.2.ffwd.net.2.weight: torch.float32\n",
      "blocks.2.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.2.ffwd.net.2.bias: torch.float32\n",
      "blocks.2.ln1.weight: torch.Size([384])\n",
      "blocks.2.ln1.weight: torch.float32\n",
      "blocks.2.ln1.bias: torch.Size([384])\n",
      "blocks.2.ln1.bias: torch.float32\n",
      "blocks.2.ln2.weight: torch.Size([384])\n",
      "blocks.2.ln2.weight: torch.float32\n",
      "blocks.2.ln2.bias: torch.Size([384])\n",
      "blocks.2.ln2.bias: torch.float32\n",
      "blocks.3.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.0.key.weight: torch.float32\n",
      "blocks.3.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.0.query.weight: torch.float32\n",
      "blocks.3.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.0.value.weight: torch.float32\n",
      "blocks.3.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.1.key.weight: torch.float32\n",
      "blocks.3.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.1.query.weight: torch.float32\n",
      "blocks.3.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.1.value.weight: torch.float32\n",
      "blocks.3.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.2.key.weight: torch.float32\n",
      "blocks.3.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.2.query.weight: torch.float32\n",
      "blocks.3.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.2.value.weight: torch.float32\n",
      "blocks.3.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.3.key.weight: torch.float32\n",
      "blocks.3.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.3.query.weight: torch.float32\n",
      "blocks.3.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.3.value.weight: torch.float32\n",
      "blocks.3.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.4.key.weight: torch.float32\n",
      "blocks.3.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.4.query.weight: torch.float32\n",
      "blocks.3.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.4.value.weight: torch.float32\n",
      "blocks.3.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.5.key.weight: torch.float32\n",
      "blocks.3.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.5.query.weight: torch.float32\n",
      "blocks.3.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.3.sa.heads.5.value.weight: torch.float32\n",
      "blocks.3.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.3.sa.proj.weight: torch.float32\n",
      "blocks.3.sa.proj.bias: torch.Size([384])\n",
      "blocks.3.sa.proj.bias: torch.float32\n",
      "blocks.3.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.3.ffwd.net.0.weight: torch.float32\n",
      "blocks.3.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.3.ffwd.net.0.bias: torch.float32\n",
      "blocks.3.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.3.ffwd.net.2.weight: torch.float32\n",
      "blocks.3.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.3.ffwd.net.2.bias: torch.float32\n",
      "blocks.3.ln1.weight: torch.Size([384])\n",
      "blocks.3.ln1.weight: torch.float32\n",
      "blocks.3.ln1.bias: torch.Size([384])\n",
      "blocks.3.ln1.bias: torch.float32\n",
      "blocks.3.ln2.weight: torch.Size([384])\n",
      "blocks.3.ln2.weight: torch.float32\n",
      "blocks.3.ln2.bias: torch.Size([384])\n",
      "blocks.3.ln2.bias: torch.float32\n",
      "blocks.4.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.0.key.weight: torch.float32\n",
      "blocks.4.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.0.query.weight: torch.float32\n",
      "blocks.4.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.0.value.weight: torch.float32\n",
      "blocks.4.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.1.key.weight: torch.float32\n",
      "blocks.4.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.1.query.weight: torch.float32\n",
      "blocks.4.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.1.value.weight: torch.float32\n",
      "blocks.4.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.2.key.weight: torch.float32\n",
      "blocks.4.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.2.query.weight: torch.float32\n",
      "blocks.4.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.2.value.weight: torch.float32\n",
      "blocks.4.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.3.key.weight: torch.float32\n",
      "blocks.4.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.3.query.weight: torch.float32\n",
      "blocks.4.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.3.value.weight: torch.float32\n",
      "blocks.4.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.4.key.weight: torch.float32\n",
      "blocks.4.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.4.query.weight: torch.float32\n",
      "blocks.4.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.4.value.weight: torch.float32\n",
      "blocks.4.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.5.key.weight: torch.float32\n",
      "blocks.4.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.5.query.weight: torch.float32\n",
      "blocks.4.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.4.sa.heads.5.value.weight: torch.float32\n",
      "blocks.4.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.4.sa.proj.weight: torch.float32\n",
      "blocks.4.sa.proj.bias: torch.Size([384])\n",
      "blocks.4.sa.proj.bias: torch.float32\n",
      "blocks.4.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.4.ffwd.net.0.weight: torch.float32\n",
      "blocks.4.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.4.ffwd.net.0.bias: torch.float32\n",
      "blocks.4.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.4.ffwd.net.2.weight: torch.float32\n",
      "blocks.4.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.4.ffwd.net.2.bias: torch.float32\n",
      "blocks.4.ln1.weight: torch.Size([384])\n",
      "blocks.4.ln1.weight: torch.float32\n",
      "blocks.4.ln1.bias: torch.Size([384])\n",
      "blocks.4.ln1.bias: torch.float32\n",
      "blocks.4.ln2.weight: torch.Size([384])\n",
      "blocks.4.ln2.weight: torch.float32\n",
      "blocks.4.ln2.bias: torch.Size([384])\n",
      "blocks.4.ln2.bias: torch.float32\n",
      "blocks.5.sa.heads.0.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.0.key.weight: torch.float32\n",
      "blocks.5.sa.heads.0.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.0.query.weight: torch.float32\n",
      "blocks.5.sa.heads.0.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.0.value.weight: torch.float32\n",
      "blocks.5.sa.heads.1.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.1.key.weight: torch.float32\n",
      "blocks.5.sa.heads.1.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.1.query.weight: torch.float32\n",
      "blocks.5.sa.heads.1.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.1.value.weight: torch.float32\n",
      "blocks.5.sa.heads.2.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.2.key.weight: torch.float32\n",
      "blocks.5.sa.heads.2.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.2.query.weight: torch.float32\n",
      "blocks.5.sa.heads.2.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.2.value.weight: torch.float32\n",
      "blocks.5.sa.heads.3.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.3.key.weight: torch.float32\n",
      "blocks.5.sa.heads.3.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.3.query.weight: torch.float32\n",
      "blocks.5.sa.heads.3.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.3.value.weight: torch.float32\n",
      "blocks.5.sa.heads.4.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.4.key.weight: torch.float32\n",
      "blocks.5.sa.heads.4.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.4.query.weight: torch.float32\n",
      "blocks.5.sa.heads.4.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.4.value.weight: torch.float32\n",
      "blocks.5.sa.heads.5.key.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.5.key.weight: torch.float32\n",
      "blocks.5.sa.heads.5.query.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.5.query.weight: torch.float32\n",
      "blocks.5.sa.heads.5.value.weight: torch.Size([64, 384])\n",
      "blocks.5.sa.heads.5.value.weight: torch.float32\n",
      "blocks.5.sa.proj.weight: torch.Size([384, 384])\n",
      "blocks.5.sa.proj.weight: torch.float32\n",
      "blocks.5.sa.proj.bias: torch.Size([384])\n",
      "blocks.5.sa.proj.bias: torch.float32\n",
      "blocks.5.ffwd.net.0.weight: torch.Size([1536, 384])\n",
      "blocks.5.ffwd.net.0.weight: torch.float32\n",
      "blocks.5.ffwd.net.0.bias: torch.Size([1536])\n",
      "blocks.5.ffwd.net.0.bias: torch.float32\n",
      "blocks.5.ffwd.net.2.weight: torch.Size([384, 1536])\n",
      "blocks.5.ffwd.net.2.weight: torch.float32\n",
      "blocks.5.ffwd.net.2.bias: torch.Size([384])\n",
      "blocks.5.ffwd.net.2.bias: torch.float32\n",
      "blocks.5.ln1.weight: torch.Size([384])\n",
      "blocks.5.ln1.weight: torch.float32\n",
      "blocks.5.ln1.bias: torch.Size([384])\n",
      "blocks.5.ln1.bias: torch.float32\n",
      "blocks.5.ln2.weight: torch.Size([384])\n",
      "blocks.5.ln2.weight: torch.float32\n",
      "blocks.5.ln2.bias: torch.Size([384])\n",
      "blocks.5.ln2.bias: torch.float32\n",
      "ln_f.weight: torch.Size([384])\n",
      "ln_f.weight: torch.float32\n",
      "ln_f.bias: torch.Size([384])\n",
      "ln_f.bias: torch.float32\n",
      "lm_head.weight: torch.Size([65, 384])\n",
      "lm_head.weight: torch.float32\n",
      "lm_head.bias: torch.Size([65])\n",
      "lm_head.bias: torch.float32\n",
      "torch.Size([65, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.0091,  0.0415, -0.1269,  ..., -0.0714,  0.0073,  0.0025],\n",
       "                      [ 0.0430,  0.0210, -0.0139,  ..., -0.0139, -0.0091,  0.0034],\n",
       "                      [-0.0393,  0.0236,  0.0226,  ...,  0.0451, -0.0622, -0.0174],\n",
       "                      ...,\n",
       "                      [ 0.0940,  0.0103,  0.1219,  ..., -0.0038,  0.0331,  0.0075],\n",
       "                      [ 0.0578, -0.0447,  0.0744,  ..., -0.0093, -0.0104,  0.0081],\n",
       "                      [ 0.0590, -0.0205, -0.0779,  ..., -0.0641,  0.0432,  0.0550]],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all parameters\n",
    "print(\"Model Parameters:\")\n",
    "for name, param in list(model.named_parameters())[:]:\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "print(model._modules[\"token_embedding_table\"].state_dict()[\"weight\"].shape)\n",
    "model._modules[\"token_embedding_table\"].state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print all parameters for half\n",
    "# model.half()\n",
    "# print(\"Model Parameters:\")\n",
    "# for name, param in list(model.named_parameters())[:3]:\n",
    "#     print(f\"{name}: {param.shape}\")\n",
    "#     print(f\"{name}: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model._modules[\"token_embedding_table\"].state_dict()[\"weight\"].shape)\n",
    "# model._modules[\"token_embedding_table\"].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| T.shape: (10, 20)\n",
      "ic| U_r.shape: (10, 8)\n",
      "ic| S_r.shape: (8, 8)\n",
      "ic| Vt_r.shape: (8, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0412\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original matrix\n",
    "T = np.random.rand(10, 20)\n",
    "\n",
    "# Compute SVD\n",
    "U, S, Vt = np.linalg.svd(T, full_matrices=False)\n",
    "\n",
    "# Choose rank r\n",
    "r = 8\n",
    "U_r = U[:, :r]\n",
    "S_r = np.diag(S[:r])\n",
    "Vt_r = Vt[:r, :]\n",
    "\n",
    "# Low-rank approximation\n",
    "T_r = U_r @ S_r @ Vt_r\n",
    "\n",
    "ic(T.shape)\n",
    "ic(U_r.shape)\n",
    "ic(S_r.shape)\n",
    "ic(Vt_r.shape)\n",
    "mae = np.mean(np.abs(T - T_r))\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape: torch.Size([65, 384])\n",
      "U_r shape: torch.Size([65, 40])\n",
      "S_r shape: torch.Size([40, 40])\n",
      "V_r shape: torch.Size([384, 40])\n",
      "Mean Absolute Error (MAE): 0.0126\n"
     ]
    }
   ],
   "source": [
    "# Original matrix\n",
    "T = model._modules[\"token_embedding_table\"].state_dict()[\"weight\"]\n",
    "\n",
    "# Compute SVD\n",
    "U, S, V = torch.svd(T)\n",
    "\n",
    "# Choose rank r\n",
    "r = 40 \n",
    "U_r = U[:, :r]\n",
    "S_r = torch.diag(S[:r])\n",
    "V_r = V[:, :r]\n",
    "\n",
    "# Low-rank approximation\n",
    "T_r = U_r @ S_r @ V_r.T\n",
    "\n",
    "# Shapes\n",
    "m,n = T.shape\n",
    "print(f\"T shape: {T.shape}\")\n",
    "print(f\"U_r shape: {U_r.shape}\")\n",
    "print(f\"S_r shape: {S_r.shape}\")\n",
    "print(f\"V_r shape: {V_r.shape}\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = torch.mean(torch.abs(T - T_r))\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.8562), 'val': tensor(1.5702)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(1.0589), 'val': tensor(1.6623)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules[\"token_embedding_table\"].state_dict()[\"weight\"][:,:] = T_r\n",
    "estimate_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_decrease(m,r,n):\n",
    "    return (m*r + r*n + r*r)/(m*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836538461538461"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_decrease(m,r,n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
